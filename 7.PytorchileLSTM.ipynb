{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9744a9f5",
   "metadata": {},
   "source": [
    "### 7.2 Veri seti hazirlanmasi ve Ön işleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831711c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nproblem tanimi: lstm ile metin turetme\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "problem tanimi: lstm ile metin turetme\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33f2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter # kelime frekanslarini hesaplamak icin \n",
    "from itertools import product # grid search icin kombinasyon olusturmak\n",
    "\n",
    "# veri yukleme ve on isleme (preprocessing)\n",
    "# urun yorumlari\n",
    "text = \"\"\"Bu ürün beklentimi fazlasıyla karşıladı.  \n",
    "Malzeme kalitesi gerçekten çok iyi.  \n",
    "Kargo hızlı ve sorunsuz bir şekilde elime ulaştı.  \n",
    "Fiyatına göre performansı harika.  \n",
    "Kesinlikle tavsiye ederim ve öneririm!\"\"\"\n",
    "\n",
    "# veri on ısleme: \n",
    "# noktalama isaretlerinden kurtul, \n",
    "# kucuk harf donusumu\n",
    "# kelimeleri bol\n",
    "\n",
    "words = text.replace(\".\", \"\").replace(\"!\",\"\").lower().split()\n",
    "\n",
    "# kelime frekanslarini hesapla ve indeksleme olustur\n",
    "word_counts = Counter(words)\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse = True) # kelime frekansini buyukten kucuge sirala\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "# egitim verisi hazirlama\n",
    "data = [(words[i], words[i+1]) for i in range(len(words) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55dc9b7",
   "metadata": {},
   "source": [
    "#### 7.3 Modelin Oluşturulması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "\n",
    "        super(LSTM, self).__init__()# bir ust sinifin constructor ini cagirma\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding katmani\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim) # LSTM katmani \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input -> embedding -> lstm -> fc -> output\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x.view(1,1,-1))\n",
    "        output = self.fc(lstm_out.view(1,-1))\n",
    "\n",
    "        return output\n",
    "        \n",
    "# model = LSTM(len(vocab), embedding_dim=8, hidden_dim=32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554d24f",
   "metadata": {},
   "source": [
    "#### 7.4 Hiperparametre Ayarlanmasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc585127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning basliyor ...\n",
      "Deneme: Embedding: 8, Hidden: 32, learning_rate: 0.01\n",
      "Epoch: 0, Loss: 86.69640\n",
      "Epoch: 10, Loss: 3.77524\n",
      "Epoch: 20, Loss: 2.30615\n",
      "Epoch: 30, Loss: 2.02073\n",
      "Epoch: 40, Loss: 1.89542\n",
      "\n",
      "Deneme: Embedding: 8, Hidden: 32, learning_rate: 0.005\n",
      "Epoch: 0, Loss: 85.57802\n",
      "Epoch: 10, Loss: 13.83929\n",
      "Epoch: 20, Loss: 3.61550\n",
      "Epoch: 30, Loss: 2.47080\n",
      "Epoch: 40, Loss: 2.10286\n",
      "\n",
      "Deneme: Embedding: 8, Hidden: 64, learning_rate: 0.01\n",
      "Epoch: 0, Loss: 86.64381\n",
      "Epoch: 10, Loss: 2.72745\n",
      "Epoch: 20, Loss: 2.09292\n",
      "Epoch: 30, Loss: 1.90552\n",
      "Epoch: 40, Loss: 1.81623\n",
      "\n",
      "Deneme: Embedding: 8, Hidden: 64, learning_rate: 0.005\n",
      "Epoch: 0, Loss: 85.89699\n",
      "Epoch: 10, Loss: 5.45951\n",
      "Epoch: 20, Loss: 2.43310\n",
      "Epoch: 30, Loss: 2.04392\n",
      "Epoch: 40, Loss: 1.89482\n",
      "\n",
      "Deneme: Embedding: 16, Hidden: 32, learning_rate: 0.01\n",
      "Epoch: 0, Loss: 85.73782\n",
      "Epoch: 10, Loss: 3.15460\n",
      "Epoch: 20, Loss: 2.16247\n",
      "Epoch: 30, Loss: 1.94681\n",
      "Epoch: 40, Loss: 1.85349\n",
      "\n",
      "Deneme: Embedding: 16, Hidden: 32, learning_rate: 0.005\n",
      "Epoch: 0, Loss: 85.40544\n",
      "Epoch: 10, Loss: 8.14118\n",
      "Epoch: 20, Loss: 2.80914\n",
      "Epoch: 30, Loss: 2.16496\n",
      "Epoch: 40, Loss: 1.94047\n",
      "\n",
      "Deneme: Embedding: 16, Hidden: 64, learning_rate: 0.01\n",
      "Epoch: 0, Loss: 86.92589\n",
      "Epoch: 10, Loss: 2.44230\n",
      "Epoch: 20, Loss: 1.99617\n",
      "Epoch: 30, Loss: 1.85761\n",
      "Epoch: 40, Loss: 1.78199\n",
      "\n",
      "Deneme: Embedding: 16, Hidden: 64, learning_rate: 0.005\n",
      "Epoch: 0, Loss: 85.27404\n",
      "Epoch: 10, Loss: 3.36480\n",
      "Epoch: 20, Loss: 2.09825\n",
      "Epoch: 30, Loss: 1.87009\n",
      "Epoch: 40, Loss: 1.77399\n",
      "\n",
      "Best params: {'embedding_dim': 16, 'hidden_dim': 64, 'learning_rate': 0.005}\n"
     ]
    }
   ],
   "source": [
    "def prepare_squence(seq, to_ix):\n",
    "    return torch.tensor([to_ix[w] for w in seq], dtype = torch.long)\n",
    "\n",
    "# hyperparameter tuning kombinasyonlrini belirle \n",
    "embedding_sizes = [8,16]\n",
    "hidden_sizes = [32, 64]\n",
    "learning_rates = [0.01, 0.005]\n",
    "best_loss = float(\"inf\") # en buyuk kayip degerini saklamk icin bir degisken \n",
    "best_params = {} # en iyi parametrelerii saklamak icin bos bir dictionary \n",
    "\n",
    "print(\"Hyperparameter tuning basliyor ...\")\n",
    "\n",
    "# grid search \n",
    "for emb_size, hidden_size, lr in product(embedding_sizes, hidden_sizes, learning_rates):\n",
    "    print(f\"Deneme: Embedding: {emb_size}, Hidden: {hidden_size}, learning_rate: {lr}\")\n",
    "    \n",
    "    # modeli tanimla \n",
    "    model = LSTM(len(vocab), emb_size, hidden_size) # secilen params ile model olsutur \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    epochs = 50\n",
    "    total_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0 # epoch baslangıcnda kaybi sıfırlayalım \n",
    "        for word, next_word in data:\n",
    "            model.zero_grad() # gradyanlari sıfırla \n",
    "            input_tensor = prepare_squence([word], word_to_ix) #girdiyi tensor e cevir \n",
    "            target_tensor  = prepare_squence([next_word], word_to_ix) #3 hedef kelimeyi tensore cevir \n",
    "            output = model(input_tensor) # prediction \n",
    "            loss = loss_func(output, target_tensor)\n",
    "            loss.backward() # geri yayilim islmei uygula \n",
    "            optimizer.step() # parametreleri guncelle \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0 : \n",
    "            print (f\"Epoch: { epoch}, Loss: { epoch_loss:.5f}\")\n",
    "        total_loss = epoch_loss\n",
    "\n",
    "    # en iyi modeli kaydet \n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_params = { \"embedding_dim\": emb_size, \"hidden_dim\": hidden_size, \"learning_rate\": lr} \n",
    "    print()\n",
    "print(f\"Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263eb61",
   "metadata": {},
   "source": [
    "#### 7.5 Model Egitimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0ec4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model training\n",
      "Final Model Epoch: 0, Loss: 85.57164\n",
      "Final Model Epoch: 10, Loss: 3.62675\n",
      "Final Model Epoch: 20, Loss: 2.20463\n",
      "Final Model Epoch: 30, Loss: 1.95314\n",
      "Final Model Epoch: 40, Loss: 1.84174\n",
      "Final Model Epoch: 50, Loss: 1.78036\n",
      "Final Model Epoch: 60, Loss: 1.73406\n",
      "Final Model Epoch: 70, Loss: 1.70200\n",
      "Final Model Epoch: 80, Loss: 1.67694\n",
      "Final Model Epoch: 90, Loss: 1.65515\n"
     ]
    }
   ],
   "source": [
    "# lstm training\n",
    "\n",
    "final_model = LSTM(len(vocab), best_params['embedding_dim'], best_params['hidden_dim'])\n",
    "optimizer = optim.Adam(final_model.parameters(), lr = best_params['learning_rate'])\n",
    "loss_function = nn.CrossEntropyLoss() # entropi kayip fonksiyonu\n",
    "\n",
    "print(\"Final model training\")\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0 \n",
    "    for word, next_word in data:\n",
    "        final_model.zero_grad()\n",
    "        input_tensor = prepare_squence([word], word_to_ix)\n",
    "        target_tensor = prepare_squence([next_word], word_to_ix)\n",
    "        output = final_model(input_tensor)\n",
    "        loss = loss_function(output, target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Final Model Epoch: {epoch}, Loss: {epoch_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58be193",
   "metadata": {},
   "source": [
    "#### 7.6 Modelin Test edilmesi ve Degerlendirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8db1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ve sorunsuz bir\n"
     ]
    }
   ],
   "source": [
    "# test ve degerlendirme\n",
    "\n",
    "# kelime tahmini fonksiyonu: baslangic kelimesi ve n adet kelime uretmesini sagla\n",
    "def predict_sequence(start_word, num_words):\n",
    "    current_word = start_word # suanki kelime baslangic kelimesi olarak ayarlanir\n",
    "    output_sequence = [current_word] # cikti dizisi \n",
    "    \n",
    "    for _ in range(num_words): # belirtilen sayida kelime tahmini\n",
    "        with torch.no_grad(): # gradyan hesaplamasi yapmadan\n",
    "            input_tensor = prepare_squence([current_word], word_to_ix) # kelime -> tensor\n",
    "            output = final_model(input_tensor)\n",
    "            predicted_idx = torch.argmax(output).item() # en yuksek olasiliga sahip kelimenin indexi\n",
    "            predicted_word = ix_to_word[predicted_idx] # indekse karsilik gelen kelimeyi return eder\n",
    "            output_sequence.append(predicted_word)\n",
    "            current_word = predicted_word # bir sonraki tahmin icin mevcut kelimeleri guncelle\n",
    "    return output_sequence # tahmin edilen kelime dizisi return edilir.\n",
    "   \n",
    "start_word = \"ve\"           \n",
    "num_predictions = 2\n",
    "predicted_sequence = predict_sequence(start_word, num_predictions)\n",
    "print(\" \".join(predicted_sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
