{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b050de",
   "metadata": {},
   "source": [
    "### 13.1 DQL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6f87e",
   "metadata": {},
   "source": [
    "![ScreenS/13_DQL.PNG](ScreenS/13_DQL.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b584b",
   "metadata": {},
   "source": [
    "#### 13.2 Replay Memory Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a78e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mfurk\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (2.2.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mfurk\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\mfurk\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "   ---------------------------------------- 0.0/965.4 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 524.3/965.4 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 965.4/965.4 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "deep q learning \n",
    "cartpole \n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\"\"\"\n",
    "!pip install gymnasium\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "Transition = namedtuple(\"Transition\", \n",
    "                        (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "# replay memory olusturma\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7174fc",
   "metadata": {},
   "source": [
    "#### 13.3 DQL Modeli Oluşturma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c88aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c5518",
   "metadata": {},
   "source": [
    "#### 13.4 Hyperparam ve Yardimci Fonk. Belirlenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e00442",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "gamma = 0.99 # discount factor\n",
    "eps_start = 0.9\n",
    "eps_end = 0.05\n",
    "eps_decay = 1000 \n",
    "tau = 0.005 # update rate of target network\n",
    "lr = 1e-4\n",
    "\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_end + (eps_start - eps_end) * \\\n",
    "        math.exp(-1. * steps_done / eps_decay)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "   \n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result = False):\n",
    "    \n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype = torch.float)\n",
    "    if show_result:\n",
    "        plt.title(\"Result\")\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(\"Training ...\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Duration\")\n",
    "    plt.plot(durations_t.numpy())\n",
    "    \n",
    "    if len(durations_t) > 100:\n",
    "        means = durations_t.unfold(0,100,1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    plt.pause(0.001)\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait = True)\n",
    "\n",
    "def optimize_model():\n",
    "    \n",
    "    \n",
    "    # hafizadda yeterli sayida deneyim var mi yok mu kontrol et, yoksa fonksiyondan cik\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    # hafizadan rastgele bir grup deneyim ornegi alinir\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # ayirma islemi\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # sonraki durumlari none olmayan bir boolean maskesi olusturur\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device = device, dtype = torch.bool)\n",
    "    # terminal olmayan tum durumlari tek bir tensor olarak birlestirir\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    # grup icindeki state, action ve reward birlestirilir\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    \n",
    "    expected_state_action_values = (next_state_values*gamma) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721dc54c",
   "metadata": {},
   "source": [
    "#### 13.5 Modelin Egitimi ve Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ba442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_episodes = 400\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # reset env\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype = torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    for t in count():\n",
    "        action = select_action(state) # action secme\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated # kaybettik yada yandik        \n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype = torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "        # stransitonlari memory de depola\n",
    "        memory.push(state, action, next_state, reward)\n",
    "            \n",
    "        # state guncelle\n",
    "        state = next_state\n",
    "        \n",
    "        # training \n",
    "        optimize_model()\n",
    "        \n",
    "        # update network parameters\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        \n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "        \n",
    "print(\"Done\")\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
